{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Federated_Learning_MNIST.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/helenabarmer/60DaysOfUdacity/blob/master/Federated_Learning_MNIST.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wuiQYJR9TNRi",
        "colab_type": "code",
        "outputId": "e448966c-bf9a-4783-bc60-6e451fb22f93",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "#Install PySyft in Google Colab\n",
        "\n",
        "!pip install tf-encrypted\n",
        "\n",
        "! URL=\"https://github.com/openmined/PySyft.git\" && FOLDER=\"PySyft\" && if [ ! -d $FOLDER ]; then git clone -b dev --single-branch $URL; else (cd $FOLDER && git pull $URL && cd ..); fi;\n",
        "\n",
        "!cd PySyft; python setup.py install  > /dev/null\n",
        "\n",
        "import os\n",
        "import sys\n",
        "module_path = os.path.abspath(os.path.join('./PySyft'))\n",
        "if module_path not in sys.path:\n",
        "    sys.path.append(module_path)\n",
        "    \n",
        "!pip install --upgrade --force-reinstall lz4\n",
        "!pip install --upgrade --force-reinstall websocket\n",
        "!pip install --upgrade --force-reinstall websockets\n",
        "!pip install --upgrade --force-reinstall zstd"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tf-encrypted in /usr/local/lib/python3.6/dist-packages (0.5.7)\n",
            "Requirement already satisfied: tensorflow<2,>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted) (1.14.0)\n",
            "Requirement already satisfied: numpy>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted) (1.16.4)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.6/dist-packages (from tf-encrypted) (5.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.12.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.33.4)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.7.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (3.7.1)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.11.2)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.15.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.8.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.1.7)\n",
            "Requirement already satisfied: keras-applications>=1.0.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.0.8)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (0.2.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.1.0)\n",
            "Requirement already satisfied: tensorflow-estimator<1.15.0rc0,>=1.14.0rc0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.14.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.1.0)\n",
            "Requirement already satisfied: tensorboard<1.15.0,>=1.14.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow<2,>=1.12.0->tf-encrypted) (1.14.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.6.1->tensorflow<2,>=1.12.0->tf-encrypted) (41.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from keras-applications>=1.0.6->tensorflow<2,>=1.12.0->tf-encrypted) (2.8.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted) (0.15.5)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<1.15.0,>=1.14.0->tensorflow<2,>=1.12.0->tf-encrypted) (3.1.1)\n",
            "From https://github.com/openmined/PySyft\n",
            " * branch              HEAD       -> FETCH_HEAD\n",
            "Already up to date.\n",
            "zip_safe flag not set; analyzing archive contents...\n",
            "Collecting lz4\n",
            "  Using cached https://files.pythonhosted.org/packages/0a/c6/96bbb3525a63ebc53ea700cc7d37ab9045542d33b4d262d0f0408ad9bbf2/lz4-2.1.10-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: syft 0.1.22a1 has requirement msgpack>=0.6.1, but you'll have msgpack 0.5.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.1.22a1 has requirement tf_encrypted!=0.5.7,>=0.5.4, but you'll have tf-encrypted 0.5.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: lz4\n",
            "  Found existing installation: lz4 2.1.10\n",
            "    Uninstalling lz4-2.1.10:\n",
            "      Successfully uninstalled lz4-2.1.10\n",
            "Successfully installed lz4-2.1.10\n",
            "Collecting websocket\n",
            "Collecting greenlet (from websocket)\n",
            "  Using cached https://files.pythonhosted.org/packages/bf/45/142141aa47e01a5779f0fa5a53b81f8379ce8f2b1cd13df7d2f1d751ae42/greenlet-0.4.15-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Collecting gevent (from websocket)\n",
            "  Using cached https://files.pythonhosted.org/packages/f2/ca/5b5962361ed832847b6b2f9a2d0452c8c2f29a93baef850bb8ad067c7bf9/gevent-1.4.0-cp36-cp36m-manylinux1_x86_64.whl\n",
            "Installing collected packages: greenlet, gevent, websocket\n",
            "  Found existing installation: greenlet 0.4.15\n",
            "    Uninstalling greenlet-0.4.15:\n",
            "      Successfully uninstalled greenlet-0.4.15\n",
            "  Found existing installation: gevent 1.4.0\n",
            "    Uninstalling gevent-1.4.0:\n",
            "      Successfully uninstalled gevent-1.4.0\n",
            "  Found existing installation: websocket 0.2.1\n",
            "    Uninstalling websocket-0.2.1:\n",
            "      Successfully uninstalled websocket-0.2.1\n",
            "Successfully installed gevent-1.4.0 greenlet-0.4.15 websocket-0.2.1\n",
            "Collecting websockets\n",
            "  Using cached https://files.pythonhosted.org/packages/f0/4b/ad228451b1c071c5c52616b7d4298ebcfcac5ae8515ede959db19e4cd56d/websockets-8.0.2-cp36-cp36m-manylinux1_x86_64.whl\n",
            "\u001b[31mERROR: syft 0.1.22a1 has requirement msgpack>=0.6.1, but you'll have msgpack 0.5.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.1.22a1 has requirement tf_encrypted!=0.5.7,>=0.5.4, but you'll have tf-encrypted 0.5.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: websockets\n",
            "  Found existing installation: websockets 8.0.2\n",
            "    Uninstalling websockets-8.0.2:\n",
            "      Successfully uninstalled websockets-8.0.2\n",
            "Successfully installed websockets-8.0.2\n",
            "Collecting zstd\n",
            "\u001b[31mERROR: syft 0.1.22a1 has requirement msgpack>=0.6.1, but you'll have msgpack 0.5.6 which is incompatible.\u001b[0m\n",
            "\u001b[31mERROR: syft 0.1.22a1 has requirement tf_encrypted!=0.5.7,>=0.5.4, but you'll have tf-encrypted 0.5.7 which is incompatible.\u001b[0m\n",
            "Installing collected packages: zstd\n",
            "  Found existing installation: zstd 1.4.1.0\n",
            "    Uninstalling zstd-1.4.1.0:\n",
            "      Successfully uninstalled zstd-1.4.1.0\n",
            "Successfully installed zstd-1.4.1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2K4TFx-hpe4V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Thanks to https://colab.research.google.com/github/chokkan/deeplearningclass/blob/master/mnist.ipynb#scrollTo=Pr_UACmc3Qgc\n",
        "import gzip\n",
        "import os\n",
        "import struct\n",
        "import numpy as np\n",
        "\n",
        "def read_image(fi):\n",
        "    magic, n, rows, columns = struct.unpack(\">IIII\", fi.read(16))\n",
        "    assert magic == 0x00000803\n",
        "    assert rows == 28\n",
        "    assert columns == 28\n",
        "    rawbuffer = fi.read()\n",
        "    assert len(rawbuffer) == n * rows * columns\n",
        "    rawdata = np.frombuffer(rawbuffer, dtype='>u1', count=n*rows*columns)\n",
        "    return rawdata.reshape(n, rows, columns).astype(np.float32) / 255.0\n",
        "\n",
        "def read_label(fi):\n",
        "    magic, n = struct.unpack(\">II\", fi.read(8))\n",
        "    assert magic == 0x00000801\n",
        "    rawbuffer = fi.read()\n",
        "    assert len(rawbuffer) == n\n",
        "    return np.frombuffer(rawbuffer, dtype='>u1', count=n)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    os.system('wget -N http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz')\n",
        "    os.system('wget -N http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz')\n",
        "    os.system('wget -N http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz')\n",
        "    os.system('wget -N http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz')\n",
        "    \n",
        "    np.savez_compressed(\n",
        "        'mnist',\n",
        "        train_images=read_image(gzip.open('train-images-idx3-ubyte.gz', 'rb')),\n",
        "        train_labels=read_label(gzip.open('train-labels-idx1-ubyte.gz', 'rb')),\n",
        "        test_images=read_image(gzip.open('t10k-images-idx3-ubyte.gz', 'rb')),\n",
        "        test_labels=read_label(gzip.open('t10k-labels-idx1-ubyte.gz', 'rb'))\n",
        "    )"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "knmcFvPscNgg",
        "colab_type": "code",
        "outputId": "c2b483bd-fb70-4495-b095-c46dcccd7957",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 53
        }
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "import syft as sy\n",
        "\n",
        "# Hook PyTorch\n",
        "hook = sy.TorchHook(torch)\n",
        "\n",
        "# Remote workers\n",
        "ada = sy.VirtualWorker(hook, id=\"ada\")\n",
        "helena = sy.VirtualWorker(hook, id=\"helena\")\n",
        "\n",
        "# Device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(device)"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "W0804 11:35:14.979645 140337805215616 hook.py:98] Torch was already hooked... skipping hooking process\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QY21O_h0kj6u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Arguments():\n",
        "    def __init__(self):\n",
        "        self.batch_size = 64\n",
        "        self.test_batch_size = 1000\n",
        "        self.lr = 0.01\n",
        "        self.epochs = 10\n",
        "        self.seed = 1\n",
        "        self.no_cuda = False\n",
        "        self.print_every = 200\n",
        "        self.log_interval = 30\n",
        "        self.plot_every = 100\n",
        "        self.use_cuda = False\n",
        "        self.save_model = False\n",
        "        \n",
        "args = Arguments()\n",
        "torch.manual_seed(args.seed)\n",
        "use_cuda = not args.no_cuda and torch.cuda.is_available()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xeOSEQwpAklk",
        "colab_type": "code",
        "outputId": "2d519584-6a35-47af-f0d0-20e98ea8aa92",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 90
        }
      },
      "source": [
        "data = np.load('mnist.npz')\n",
        "print(data['train_images'].shape, data['train_images'].dtype)\n",
        "print(data['train_labels'].shape, data['train_labels'].dtype)\n",
        "print(data['test_images'].shape, data['test_images'].dtype)\n",
        "print(data['test_labels'].shape, data['test_labels'].dtype)"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28) float32\n",
            "(60000,) uint8\n",
            "(10000, 28, 28) float32\n",
            "(10000,) uint8\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_7PDcXlyApaD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                               transforms.Normalize((0.1307,), (0.3081,))])\n",
        "\n",
        "trainset = datasets.MNIST('../data', train=True, download=True, transform=transform)\n",
        "testset = datasets.MNIST('../data', train=False, download=True, transform=transform)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aH6Bqmh8CAWC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "federated_trainloader = sy.FederatedDataLoader(\n",
        "    trainset.federate((ada, helena)),\n",
        "    batch_size=64, shuffle=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "toMRyZ_bDhF8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "testloader = torch.utils.data.DataLoader(testset, batch_size=64, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RYmL5CxsD1mA",
        "colab_type": "code",
        "outputId": "b52c6bcf-d034-4df1-e57c-ac4aa62b475e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 108
        }
      },
      "source": [
        "class Classifier(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(Classifier, self).__init__()\n",
        "    self.fc1 = nn.Linear(784, 250)\n",
        "    self.fc2 = nn.Linear(250, 100)\n",
        "    self.fc3 = nn.Linear(100, 10)\n",
        "    \n",
        "  def forward (self, x):\n",
        "    x = x.view(x.shape[0], -1)\n",
        "    x = F.relu(self.fc1(x))\n",
        "    x = F.relu(self.fc2(x))\n",
        "    x = self.fc3(x)\n",
        "    return F.log_softmax(x, dim =1)\n",
        "  \n",
        "model = Classifier()\n",
        "print(model)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Classifier(\n",
            "  (fc1): Linear(in_features=784, out_features=250, bias=True)\n",
            "  (fc2): Linear(in_features=250, out_features=100, bias=True)\n",
            "  (fc3): Linear(in_features=100, out_features=10, bias=True)\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AZ0U3Pq5Fq1f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def train (args, model, device, federated_trainloader, optimizer, epoch):\n",
        "  model.train()\n",
        "  for param in model.parameters():\n",
        "    param.requires_grad = True\n",
        "  for batch_idx, (data, target) in enumerate(federated_trainloader):\n",
        "    model.send(data.location)\n",
        "    data, target = data.to(device), target.to(device)\n",
        "    optimizer.zero_grad()\n",
        "    output = model(data)\n",
        "    loss = F.nll_loss(output, target)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "    model.get()\n",
        "    if batch_idx % args.log_interval == 0:\n",
        "        loss = loss.get()\n",
        "        \n",
        "        print('Train epoch: {} [{} / {} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
        "        epoch, batch_idx * args.batch_size, len(federated_trainloader) * args.batch_size,\n",
        "        100. * batch_idx / len(federated_trainloader), loss.item()))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cFkSE6EaPkNg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def test(args, model, device, testloader):\n",
        "  model.eval()\n",
        "  test_loss = 0\n",
        "  correct = 0\n",
        "  \n",
        "  with torch.no_grad():\n",
        "    for data, target in testloader:\n",
        "      data, target = data.to(device), target.to(device)\n",
        "      output = model(data)\n",
        "      test_loss += F.nll_loss(output, target, reduction='sum').item()\n",
        "      pred = output.argmax(1, keepdim=True)\n",
        "      correct += pred.eq(target.view_as(pred)).sum().item()\n",
        "      \n",
        "  test_loss /= len(testloader.dataset)\n",
        "  \n",
        "  print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n",
        "  test_loss, correct, len(testloader.dataset),\n",
        "  100. * correct /len(testloader.dataset)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "csL74VTA4_Mz",
        "colab_type": "code",
        "outputId": "38f1f809-6274-49d5-cf53-bbbf4e418146",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "%%time\n",
        "model.to(device)\n",
        "optimizer = optim.SGD(model.parameters(), lr=args.lr)\n",
        "\n",
        "for epoch in range(1, args.epochs + 1):\n",
        "  train(args, model, device, federated_trainloader, optimizer, epoch)\n",
        "  test(args, model, device, testloader)\n",
        "  \n",
        "if (args.save_model):\n",
        "  torch.save(model.state_dict(), \"mnist_cn.pt\")"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train epoch: 1 [0 / 60032 (0%)]\tLoss: 2.283067\n",
            "Train epoch: 1 [1920 / 60032 (3%)]\tLoss: 2.219810\n",
            "Train epoch: 1 [3840 / 60032 (6%)]\tLoss: 2.037053\n",
            "Train epoch: 1 [5760 / 60032 (10%)]\tLoss: 1.890976\n",
            "Train epoch: 1 [7680 / 60032 (13%)]\tLoss: 1.698693\n",
            "Train epoch: 1 [9600 / 60032 (16%)]\tLoss: 1.358035\n",
            "Train epoch: 1 [11520 / 60032 (19%)]\tLoss: 1.161821\n",
            "Train epoch: 1 [13440 / 60032 (22%)]\tLoss: 0.929798\n",
            "Train epoch: 1 [15360 / 60032 (26%)]\tLoss: 0.780834\n",
            "Train epoch: 1 [17280 / 60032 (29%)]\tLoss: 0.743581\n",
            "Train epoch: 1 [19200 / 60032 (32%)]\tLoss: 0.609117\n",
            "Train epoch: 1 [21120 / 60032 (35%)]\tLoss: 0.553566\n",
            "Train epoch: 1 [23040 / 60032 (38%)]\tLoss: 0.783444\n",
            "Train epoch: 1 [24960 / 60032 (42%)]\tLoss: 0.620301\n",
            "Train epoch: 1 [26880 / 60032 (45%)]\tLoss: 0.405261\n",
            "Train epoch: 1 [28800 / 60032 (48%)]\tLoss: 0.552524\n",
            "Train epoch: 1 [30720 / 60032 (51%)]\tLoss: 0.391389\n",
            "Train epoch: 1 [32640 / 60032 (54%)]\tLoss: 0.316816\n",
            "Train epoch: 1 [34560 / 60032 (58%)]\tLoss: 0.281896\n",
            "Train epoch: 1 [36480 / 60032 (61%)]\tLoss: 0.357613\n",
            "Train epoch: 1 [38400 / 60032 (64%)]\tLoss: 0.345819\n",
            "Train epoch: 1 [40320 / 60032 (67%)]\tLoss: 0.386366\n",
            "Train epoch: 1 [42240 / 60032 (70%)]\tLoss: 0.344819\n",
            "Train epoch: 1 [44160 / 60032 (74%)]\tLoss: 0.338859\n",
            "Train epoch: 1 [46080 / 60032 (77%)]\tLoss: 0.240015\n",
            "Train epoch: 1 [48000 / 60032 (80%)]\tLoss: 0.358603\n",
            "Train epoch: 1 [49920 / 60032 (83%)]\tLoss: 0.459773\n",
            "Train epoch: 1 [51840 / 60032 (86%)]\tLoss: 0.290624\n",
            "Train epoch: 1 [53760 / 60032 (90%)]\tLoss: 0.410590\n",
            "Train epoch: 1 [55680 / 60032 (93%)]\tLoss: 0.351962\n",
            "Train epoch: 1 [57600 / 60032 (96%)]\tLoss: 0.299628\n",
            "Train epoch: 1 [59520 / 60032 (99%)]\tLoss: 0.450037\n",
            "\n",
            "Test set: Average loss: 0.3322, Accuracy: 9029/10000 (90%)\n",
            "\n",
            "Train epoch: 2 [0 / 60032 (0%)]\tLoss: 0.271906\n",
            "Train epoch: 2 [1920 / 60032 (3%)]\tLoss: 0.291983\n",
            "Train epoch: 2 [3840 / 60032 (6%)]\tLoss: 0.326651\n",
            "Train epoch: 2 [5760 / 60032 (10%)]\tLoss: 0.319846\n",
            "Train epoch: 2 [7680 / 60032 (13%)]\tLoss: 0.341155\n",
            "Train epoch: 2 [9600 / 60032 (16%)]\tLoss: 0.282526\n",
            "Train epoch: 2 [11520 / 60032 (19%)]\tLoss: 0.330290\n",
            "Train epoch: 2 [13440 / 60032 (22%)]\tLoss: 0.446429\n",
            "Train epoch: 2 [15360 / 60032 (26%)]\tLoss: 0.277514\n",
            "Train epoch: 2 [17280 / 60032 (29%)]\tLoss: 0.152962\n",
            "Train epoch: 2 [19200 / 60032 (32%)]\tLoss: 0.602749\n",
            "Train epoch: 2 [21120 / 60032 (35%)]\tLoss: 0.264769\n",
            "Train epoch: 2 [23040 / 60032 (38%)]\tLoss: 0.264722\n",
            "Train epoch: 2 [24960 / 60032 (42%)]\tLoss: 0.389232\n",
            "Train epoch: 2 [26880 / 60032 (45%)]\tLoss: 0.361686\n",
            "Train epoch: 2 [28800 / 60032 (48%)]\tLoss: 0.352582\n",
            "Train epoch: 2 [30720 / 60032 (51%)]\tLoss: 0.352749\n",
            "Train epoch: 2 [32640 / 60032 (54%)]\tLoss: 0.272225\n",
            "Train epoch: 2 [34560 / 60032 (58%)]\tLoss: 0.256145\n",
            "Train epoch: 2 [36480 / 60032 (61%)]\tLoss: 0.449705\n",
            "Train epoch: 2 [38400 / 60032 (64%)]\tLoss: 0.421885\n",
            "Train epoch: 2 [40320 / 60032 (67%)]\tLoss: 0.276684\n",
            "Train epoch: 2 [42240 / 60032 (70%)]\tLoss: 0.449237\n",
            "Train epoch: 2 [44160 / 60032 (74%)]\tLoss: 0.227752\n",
            "Train epoch: 2 [46080 / 60032 (77%)]\tLoss: 0.191232\n",
            "Train epoch: 2 [48000 / 60032 (80%)]\tLoss: 0.301094\n",
            "Train epoch: 2 [49920 / 60032 (83%)]\tLoss: 0.302540\n",
            "Train epoch: 2 [51840 / 60032 (86%)]\tLoss: 0.655308\n",
            "Train epoch: 2 [53760 / 60032 (90%)]\tLoss: 0.345218\n",
            "Train epoch: 2 [55680 / 60032 (93%)]\tLoss: 0.493326\n",
            "Train epoch: 2 [57600 / 60032 (96%)]\tLoss: 0.207615\n",
            "Train epoch: 2 [59520 / 60032 (99%)]\tLoss: 0.230563\n",
            "\n",
            "Test set: Average loss: 0.2595, Accuracy: 9242/10000 (92%)\n",
            "\n",
            "Train epoch: 3 [0 / 60032 (0%)]\tLoss: 0.472560\n",
            "Train epoch: 3 [1920 / 60032 (3%)]\tLoss: 0.319806\n",
            "Train epoch: 3 [3840 / 60032 (6%)]\tLoss: 0.219288\n",
            "Train epoch: 3 [5760 / 60032 (10%)]\tLoss: 0.391645\n",
            "Train epoch: 3 [7680 / 60032 (13%)]\tLoss: 0.227091\n",
            "Train epoch: 3 [9600 / 60032 (16%)]\tLoss: 0.211831\n",
            "Train epoch: 3 [11520 / 60032 (19%)]\tLoss: 0.113716\n",
            "Train epoch: 3 [13440 / 60032 (22%)]\tLoss: 0.227148\n",
            "Train epoch: 3 [15360 / 60032 (26%)]\tLoss: 0.242247\n",
            "Train epoch: 3 [17280 / 60032 (29%)]\tLoss: 0.186696\n",
            "Train epoch: 3 [19200 / 60032 (32%)]\tLoss: 0.456395\n",
            "Train epoch: 3 [21120 / 60032 (35%)]\tLoss: 0.298890\n",
            "Train epoch: 3 [23040 / 60032 (38%)]\tLoss: 0.162926\n",
            "Train epoch: 3 [24960 / 60032 (42%)]\tLoss: 0.181420\n",
            "Train epoch: 3 [26880 / 60032 (45%)]\tLoss: 0.098859\n",
            "Train epoch: 3 [28800 / 60032 (48%)]\tLoss: 0.235103\n",
            "Train epoch: 3 [30720 / 60032 (51%)]\tLoss: 0.362173\n",
            "Train epoch: 3 [32640 / 60032 (54%)]\tLoss: 0.220564\n",
            "Train epoch: 3 [34560 / 60032 (58%)]\tLoss: 0.296444\n",
            "Train epoch: 3 [36480 / 60032 (61%)]\tLoss: 0.330034\n",
            "Train epoch: 3 [38400 / 60032 (64%)]\tLoss: 0.357917\n",
            "Train epoch: 3 [40320 / 60032 (67%)]\tLoss: 0.190299\n",
            "Train epoch: 3 [42240 / 60032 (70%)]\tLoss: 0.452090\n",
            "Train epoch: 3 [44160 / 60032 (74%)]\tLoss: 0.319549\n",
            "Train epoch: 3 [46080 / 60032 (77%)]\tLoss: 0.155144\n",
            "Train epoch: 3 [48000 / 60032 (80%)]\tLoss: 0.213353\n",
            "Train epoch: 3 [49920 / 60032 (83%)]\tLoss: 0.214735\n",
            "Train epoch: 3 [51840 / 60032 (86%)]\tLoss: 0.245058\n",
            "Train epoch: 3 [53760 / 60032 (90%)]\tLoss: 0.425060\n",
            "Train epoch: 3 [55680 / 60032 (93%)]\tLoss: 0.141098\n",
            "Train epoch: 3 [57600 / 60032 (96%)]\tLoss: 0.110949\n",
            "Train epoch: 3 [59520 / 60032 (99%)]\tLoss: 0.304689\n",
            "\n",
            "Test set: Average loss: 0.2238, Accuracy: 9337/10000 (93%)\n",
            "\n",
            "Train epoch: 4 [0 / 60032 (0%)]\tLoss: 0.178119\n",
            "Train epoch: 4 [1920 / 60032 (3%)]\tLoss: 0.202104\n",
            "Train epoch: 4 [3840 / 60032 (6%)]\tLoss: 0.347396\n",
            "Train epoch: 4 [5760 / 60032 (10%)]\tLoss: 0.435052\n",
            "Train epoch: 4 [7680 / 60032 (13%)]\tLoss: 0.283169\n",
            "Train epoch: 4 [9600 / 60032 (16%)]\tLoss: 0.209497\n",
            "Train epoch: 4 [11520 / 60032 (19%)]\tLoss: 0.235189\n",
            "Train epoch: 4 [13440 / 60032 (22%)]\tLoss: 0.189225\n",
            "Train epoch: 4 [15360 / 60032 (26%)]\tLoss: 0.305857\n",
            "Train epoch: 4 [17280 / 60032 (29%)]\tLoss: 0.210252\n",
            "Train epoch: 4 [19200 / 60032 (32%)]\tLoss: 0.259539\n",
            "Train epoch: 4 [21120 / 60032 (35%)]\tLoss: 0.090954\n",
            "Train epoch: 4 [23040 / 60032 (38%)]\tLoss: 0.252686\n",
            "Train epoch: 4 [24960 / 60032 (42%)]\tLoss: 0.178681\n",
            "Train epoch: 4 [26880 / 60032 (45%)]\tLoss: 0.168303\n",
            "Train epoch: 4 [28800 / 60032 (48%)]\tLoss: 0.145371\n",
            "Train epoch: 4 [30720 / 60032 (51%)]\tLoss: 0.125002\n",
            "Train epoch: 4 [32640 / 60032 (54%)]\tLoss: 0.276008\n",
            "Train epoch: 4 [34560 / 60032 (58%)]\tLoss: 0.129307\n",
            "Train epoch: 4 [36480 / 60032 (61%)]\tLoss: 0.094874\n",
            "Train epoch: 4 [38400 / 60032 (64%)]\tLoss: 0.225492\n",
            "Train epoch: 4 [40320 / 60032 (67%)]\tLoss: 0.166126\n",
            "Train epoch: 4 [42240 / 60032 (70%)]\tLoss: 0.150300\n",
            "Train epoch: 4 [44160 / 60032 (74%)]\tLoss: 0.227071\n",
            "Train epoch: 4 [46080 / 60032 (77%)]\tLoss: 0.111907\n",
            "Train epoch: 4 [48000 / 60032 (80%)]\tLoss: 0.087023\n",
            "Train epoch: 4 [49920 / 60032 (83%)]\tLoss: 0.237196\n",
            "Train epoch: 4 [51840 / 60032 (86%)]\tLoss: 0.247639\n",
            "Train epoch: 4 [53760 / 60032 (90%)]\tLoss: 0.210288\n",
            "Train epoch: 4 [55680 / 60032 (93%)]\tLoss: 0.162099\n",
            "Train epoch: 4 [57600 / 60032 (96%)]\tLoss: 0.317486\n",
            "Train epoch: 4 [59520 / 60032 (99%)]\tLoss: 0.265798\n",
            "\n",
            "Test set: Average loss: 0.1935, Accuracy: 9447/10000 (94%)\n",
            "\n",
            "Train epoch: 5 [0 / 60032 (0%)]\tLoss: 0.116123\n",
            "Train epoch: 5 [1920 / 60032 (3%)]\tLoss: 0.181384\n",
            "Train epoch: 5 [3840 / 60032 (6%)]\tLoss: 0.109735\n",
            "Train epoch: 5 [5760 / 60032 (10%)]\tLoss: 0.265290\n",
            "Train epoch: 5 [7680 / 60032 (13%)]\tLoss: 0.276335\n",
            "Train epoch: 5 [9600 / 60032 (16%)]\tLoss: 0.267286\n",
            "Train epoch: 5 [11520 / 60032 (19%)]\tLoss: 0.055466\n",
            "Train epoch: 5 [13440 / 60032 (22%)]\tLoss: 0.184068\n",
            "Train epoch: 5 [15360 / 60032 (26%)]\tLoss: 0.123626\n",
            "Train epoch: 5 [17280 / 60032 (29%)]\tLoss: 0.145340\n",
            "Train epoch: 5 [19200 / 60032 (32%)]\tLoss: 0.264804\n",
            "Train epoch: 5 [21120 / 60032 (35%)]\tLoss: 0.100687\n",
            "Train epoch: 5 [23040 / 60032 (38%)]\tLoss: 0.198927\n",
            "Train epoch: 5 [24960 / 60032 (42%)]\tLoss: 0.179784\n",
            "Train epoch: 5 [26880 / 60032 (45%)]\tLoss: 0.134911\n",
            "Train epoch: 5 [28800 / 60032 (48%)]\tLoss: 0.157130\n",
            "Train epoch: 5 [30720 / 60032 (51%)]\tLoss: 0.128393\n",
            "Train epoch: 5 [32640 / 60032 (54%)]\tLoss: 0.070204\n",
            "Train epoch: 5 [34560 / 60032 (58%)]\tLoss: 0.165761\n",
            "Train epoch: 5 [36480 / 60032 (61%)]\tLoss: 0.141376\n",
            "Train epoch: 5 [38400 / 60032 (64%)]\tLoss: 0.111667\n",
            "Train epoch: 5 [40320 / 60032 (67%)]\tLoss: 0.297105\n",
            "Train epoch: 5 [42240 / 60032 (70%)]\tLoss: 0.171629\n",
            "Train epoch: 5 [44160 / 60032 (74%)]\tLoss: 0.374268\n",
            "Train epoch: 5 [46080 / 60032 (77%)]\tLoss: 0.100552\n",
            "Train epoch: 5 [48000 / 60032 (80%)]\tLoss: 0.082555\n",
            "Train epoch: 5 [49920 / 60032 (83%)]\tLoss: 0.302493\n",
            "Train epoch: 5 [51840 / 60032 (86%)]\tLoss: 0.141415\n",
            "Train epoch: 5 [53760 / 60032 (90%)]\tLoss: 0.092129\n",
            "Train epoch: 5 [55680 / 60032 (93%)]\tLoss: 0.123799\n",
            "Train epoch: 5 [57600 / 60032 (96%)]\tLoss: 0.242340\n",
            "Train epoch: 5 [59520 / 60032 (99%)]\tLoss: 0.103365\n",
            "\n",
            "Test set: Average loss: 0.1711, Accuracy: 9501/10000 (95%)\n",
            "\n",
            "Train epoch: 6 [0 / 60032 (0%)]\tLoss: 0.265309\n",
            "Train epoch: 6 [1920 / 60032 (3%)]\tLoss: 0.225775\n",
            "Train epoch: 6 [3840 / 60032 (6%)]\tLoss: 0.232807\n",
            "Train epoch: 6 [5760 / 60032 (10%)]\tLoss: 0.207505\n",
            "Train epoch: 6 [7680 / 60032 (13%)]\tLoss: 0.108134\n",
            "Train epoch: 6 [9600 / 60032 (16%)]\tLoss: 0.099226\n",
            "Train epoch: 6 [11520 / 60032 (19%)]\tLoss: 0.102206\n",
            "Train epoch: 6 [13440 / 60032 (22%)]\tLoss: 0.134383\n",
            "Train epoch: 6 [15360 / 60032 (26%)]\tLoss: 0.140690\n",
            "Train epoch: 6 [17280 / 60032 (29%)]\tLoss: 0.217276\n",
            "Train epoch: 6 [19200 / 60032 (32%)]\tLoss: 0.190001\n",
            "Train epoch: 6 [21120 / 60032 (35%)]\tLoss: 0.062291\n",
            "Train epoch: 6 [23040 / 60032 (38%)]\tLoss: 0.072387\n",
            "Train epoch: 6 [24960 / 60032 (42%)]\tLoss: 0.285342\n",
            "Train epoch: 6 [26880 / 60032 (45%)]\tLoss: 0.224710\n",
            "Train epoch: 6 [28800 / 60032 (48%)]\tLoss: 0.309213\n",
            "Train epoch: 6 [30720 / 60032 (51%)]\tLoss: 0.098087\n",
            "Train epoch: 6 [32640 / 60032 (54%)]\tLoss: 0.200927\n",
            "Train epoch: 6 [34560 / 60032 (58%)]\tLoss: 0.202093\n",
            "Train epoch: 6 [36480 / 60032 (61%)]\tLoss: 0.344658\n",
            "Train epoch: 6 [38400 / 60032 (64%)]\tLoss: 0.197606\n",
            "Train epoch: 6 [40320 / 60032 (67%)]\tLoss: 0.247310\n",
            "Train epoch: 6 [42240 / 60032 (70%)]\tLoss: 0.246850\n",
            "Train epoch: 6 [44160 / 60032 (74%)]\tLoss: 0.133467\n",
            "Train epoch: 6 [46080 / 60032 (77%)]\tLoss: 0.075785\n",
            "Train epoch: 6 [48000 / 60032 (80%)]\tLoss: 0.112206\n",
            "Train epoch: 6 [49920 / 60032 (83%)]\tLoss: 0.231872\n",
            "Train epoch: 6 [51840 / 60032 (86%)]\tLoss: 0.242145\n",
            "Train epoch: 6 [53760 / 60032 (90%)]\tLoss: 0.233660\n",
            "Train epoch: 6 [55680 / 60032 (93%)]\tLoss: 0.150293\n",
            "Train epoch: 6 [57600 / 60032 (96%)]\tLoss: 0.097820\n",
            "Train epoch: 6 [59520 / 60032 (99%)]\tLoss: 0.178727\n",
            "\n",
            "Test set: Average loss: 0.1519, Accuracy: 9546/10000 (95%)\n",
            "\n",
            "Train epoch: 7 [0 / 60032 (0%)]\tLoss: 0.115907\n",
            "Train epoch: 7 [1920 / 60032 (3%)]\tLoss: 0.124634\n",
            "Train epoch: 7 [3840 / 60032 (6%)]\tLoss: 0.109459\n",
            "Train epoch: 7 [5760 / 60032 (10%)]\tLoss: 0.145058\n",
            "Train epoch: 7 [7680 / 60032 (13%)]\tLoss: 0.310866\n",
            "Train epoch: 7 [9600 / 60032 (16%)]\tLoss: 0.087432\n",
            "Train epoch: 7 [11520 / 60032 (19%)]\tLoss: 0.165475\n",
            "Train epoch: 7 [13440 / 60032 (22%)]\tLoss: 0.124931\n",
            "Train epoch: 7 [15360 / 60032 (26%)]\tLoss: 0.211335\n",
            "Train epoch: 7 [17280 / 60032 (29%)]\tLoss: 0.246129\n",
            "Train epoch: 7 [19200 / 60032 (32%)]\tLoss: 0.153245\n",
            "Train epoch: 7 [21120 / 60032 (35%)]\tLoss: 0.194569\n",
            "Train epoch: 7 [23040 / 60032 (38%)]\tLoss: 0.121133\n",
            "Train epoch: 7 [24960 / 60032 (42%)]\tLoss: 0.291379\n",
            "Train epoch: 7 [26880 / 60032 (45%)]\tLoss: 0.175677\n",
            "Train epoch: 7 [28800 / 60032 (48%)]\tLoss: 0.173884\n",
            "Train epoch: 7 [30720 / 60032 (51%)]\tLoss: 0.057336\n",
            "Train epoch: 7 [32640 / 60032 (54%)]\tLoss: 0.094688\n",
            "Train epoch: 7 [34560 / 60032 (58%)]\tLoss: 0.059114\n",
            "Train epoch: 7 [36480 / 60032 (61%)]\tLoss: 0.068858\n",
            "Train epoch: 7 [38400 / 60032 (64%)]\tLoss: 0.522278\n",
            "Train epoch: 7 [40320 / 60032 (67%)]\tLoss: 0.171253\n",
            "Train epoch: 7 [42240 / 60032 (70%)]\tLoss: 0.107709\n",
            "Train epoch: 7 [44160 / 60032 (74%)]\tLoss: 0.046238\n",
            "Train epoch: 7 [46080 / 60032 (77%)]\tLoss: 0.100372\n",
            "Train epoch: 7 [48000 / 60032 (80%)]\tLoss: 0.072917\n",
            "Train epoch: 7 [49920 / 60032 (83%)]\tLoss: 0.074715\n",
            "Train epoch: 7 [51840 / 60032 (86%)]\tLoss: 0.146763\n",
            "Train epoch: 7 [53760 / 60032 (90%)]\tLoss: 0.149839\n",
            "Train epoch: 7 [55680 / 60032 (93%)]\tLoss: 0.106426\n",
            "Train epoch: 7 [57600 / 60032 (96%)]\tLoss: 0.171058\n",
            "Train epoch: 7 [59520 / 60032 (99%)]\tLoss: 0.240671\n",
            "\n",
            "Test set: Average loss: 0.1389, Accuracy: 9595/10000 (96%)\n",
            "\n",
            "Train epoch: 8 [0 / 60032 (0%)]\tLoss: 0.069196\n",
            "Train epoch: 8 [1920 / 60032 (3%)]\tLoss: 0.106361\n",
            "Train epoch: 8 [3840 / 60032 (6%)]\tLoss: 0.095161\n",
            "Train epoch: 8 [5760 / 60032 (10%)]\tLoss: 0.203936\n",
            "Train epoch: 8 [7680 / 60032 (13%)]\tLoss: 0.099851\n",
            "Train epoch: 8 [9600 / 60032 (16%)]\tLoss: 0.087106\n",
            "Train epoch: 8 [11520 / 60032 (19%)]\tLoss: 0.067359\n",
            "Train epoch: 8 [13440 / 60032 (22%)]\tLoss: 0.190976\n",
            "Train epoch: 8 [15360 / 60032 (26%)]\tLoss: 0.113853\n",
            "Train epoch: 8 [17280 / 60032 (29%)]\tLoss: 0.067920\n",
            "Train epoch: 8 [19200 / 60032 (32%)]\tLoss: 0.134601\n",
            "Train epoch: 8 [21120 / 60032 (35%)]\tLoss: 0.103679\n",
            "Train epoch: 8 [23040 / 60032 (38%)]\tLoss: 0.078904\n",
            "Train epoch: 8 [24960 / 60032 (42%)]\tLoss: 0.172174\n",
            "Train epoch: 8 [26880 / 60032 (45%)]\tLoss: 0.112467\n",
            "Train epoch: 8 [28800 / 60032 (48%)]\tLoss: 0.138027\n",
            "Train epoch: 8 [30720 / 60032 (51%)]\tLoss: 0.073795\n",
            "Train epoch: 8 [32640 / 60032 (54%)]\tLoss: 0.120160\n",
            "Train epoch: 8 [34560 / 60032 (58%)]\tLoss: 0.029737\n",
            "Train epoch: 8 [36480 / 60032 (61%)]\tLoss: 0.164005\n",
            "Train epoch: 8 [38400 / 60032 (64%)]\tLoss: 0.120890\n",
            "Train epoch: 8 [40320 / 60032 (67%)]\tLoss: 0.050776\n",
            "Train epoch: 8 [42240 / 60032 (70%)]\tLoss: 0.085714\n",
            "Train epoch: 8 [44160 / 60032 (74%)]\tLoss: 0.122260\n",
            "Train epoch: 8 [46080 / 60032 (77%)]\tLoss: 0.120489\n",
            "Train epoch: 8 [48000 / 60032 (80%)]\tLoss: 0.096657\n",
            "Train epoch: 8 [49920 / 60032 (83%)]\tLoss: 0.098750\n",
            "Train epoch: 8 [51840 / 60032 (86%)]\tLoss: 0.196353\n",
            "Train epoch: 8 [53760 / 60032 (90%)]\tLoss: 0.135659\n",
            "Train epoch: 8 [55680 / 60032 (93%)]\tLoss: 0.071680\n",
            "Train epoch: 8 [57600 / 60032 (96%)]\tLoss: 0.122747\n",
            "Train epoch: 8 [59520 / 60032 (99%)]\tLoss: 0.038616\n",
            "\n",
            "Test set: Average loss: 0.1272, Accuracy: 9621/10000 (96%)\n",
            "\n",
            "Train epoch: 9 [0 / 60032 (0%)]\tLoss: 0.082670\n",
            "Train epoch: 9 [1920 / 60032 (3%)]\tLoss: 0.108391\n",
            "Train epoch: 9 [3840 / 60032 (6%)]\tLoss: 0.059097\n",
            "Train epoch: 9 [5760 / 60032 (10%)]\tLoss: 0.170415\n",
            "Train epoch: 9 [7680 / 60032 (13%)]\tLoss: 0.326491\n",
            "Train epoch: 9 [9600 / 60032 (16%)]\tLoss: 0.133189\n",
            "Train epoch: 9 [11520 / 60032 (19%)]\tLoss: 0.161900\n",
            "Train epoch: 9 [13440 / 60032 (22%)]\tLoss: 0.074935\n",
            "Train epoch: 9 [15360 / 60032 (26%)]\tLoss: 0.113389\n",
            "Train epoch: 9 [17280 / 60032 (29%)]\tLoss: 0.292259\n",
            "Train epoch: 9 [19200 / 60032 (32%)]\tLoss: 0.191259\n",
            "Train epoch: 9 [21120 / 60032 (35%)]\tLoss: 0.090123\n",
            "Train epoch: 9 [23040 / 60032 (38%)]\tLoss: 0.059462\n",
            "Train epoch: 9 [24960 / 60032 (42%)]\tLoss: 0.144818\n",
            "Train epoch: 9 [26880 / 60032 (45%)]\tLoss: 0.065851\n",
            "Train epoch: 9 [28800 / 60032 (48%)]\tLoss: 0.127517\n",
            "Train epoch: 9 [30720 / 60032 (51%)]\tLoss: 0.067828\n",
            "Train epoch: 9 [32640 / 60032 (54%)]\tLoss: 0.107155\n",
            "Train epoch: 9 [34560 / 60032 (58%)]\tLoss: 0.050678\n",
            "Train epoch: 9 [36480 / 60032 (61%)]\tLoss: 0.184352\n",
            "Train epoch: 9 [38400 / 60032 (64%)]\tLoss: 0.094636\n",
            "Train epoch: 9 [40320 / 60032 (67%)]\tLoss: 0.143193\n",
            "Train epoch: 9 [42240 / 60032 (70%)]\tLoss: 0.089907\n",
            "Train epoch: 9 [44160 / 60032 (74%)]\tLoss: 0.084439\n",
            "Train epoch: 9 [46080 / 60032 (77%)]\tLoss: 0.130273\n",
            "Train epoch: 9 [48000 / 60032 (80%)]\tLoss: 0.072451\n",
            "Train epoch: 9 [49920 / 60032 (83%)]\tLoss: 0.170588\n",
            "Train epoch: 9 [51840 / 60032 (86%)]\tLoss: 0.120098\n",
            "Train epoch: 9 [53760 / 60032 (90%)]\tLoss: 0.153352\n",
            "Train epoch: 9 [55680 / 60032 (93%)]\tLoss: 0.053327\n",
            "Train epoch: 9 [57600 / 60032 (96%)]\tLoss: 0.261209\n",
            "Train epoch: 9 [59520 / 60032 (99%)]\tLoss: 0.053185\n",
            "\n",
            "Test set: Average loss: 0.1171, Accuracy: 9648/10000 (96%)\n",
            "\n",
            "Train epoch: 10 [0 / 60032 (0%)]\tLoss: 0.087391\n",
            "Train epoch: 10 [1920 / 60032 (3%)]\tLoss: 0.092617\n",
            "Train epoch: 10 [3840 / 60032 (6%)]\tLoss: 0.129721\n",
            "Train epoch: 10 [5760 / 60032 (10%)]\tLoss: 0.222241\n",
            "Train epoch: 10 [7680 / 60032 (13%)]\tLoss: 0.163097\n",
            "Train epoch: 10 [9600 / 60032 (16%)]\tLoss: 0.097540\n",
            "Train epoch: 10 [11520 / 60032 (19%)]\tLoss: 0.042168\n",
            "Train epoch: 10 [13440 / 60032 (22%)]\tLoss: 0.052928\n",
            "Train epoch: 10 [15360 / 60032 (26%)]\tLoss: 0.064521\n",
            "Train epoch: 10 [17280 / 60032 (29%)]\tLoss: 0.171780\n",
            "Train epoch: 10 [19200 / 60032 (32%)]\tLoss: 0.143632\n",
            "Train epoch: 10 [21120 / 60032 (35%)]\tLoss: 0.223748\n",
            "Train epoch: 10 [23040 / 60032 (38%)]\tLoss: 0.157572\n",
            "Train epoch: 10 [24960 / 60032 (42%)]\tLoss: 0.063557\n",
            "Train epoch: 10 [26880 / 60032 (45%)]\tLoss: 0.121825\n",
            "Train epoch: 10 [28800 / 60032 (48%)]\tLoss: 0.101463\n",
            "Train epoch: 10 [30720 / 60032 (51%)]\tLoss: 0.153587\n",
            "Train epoch: 10 [32640 / 60032 (54%)]\tLoss: 0.112906\n",
            "Train epoch: 10 [34560 / 60032 (58%)]\tLoss: 0.151524\n",
            "Train epoch: 10 [36480 / 60032 (61%)]\tLoss: 0.136469\n",
            "Train epoch: 10 [38400 / 60032 (64%)]\tLoss: 0.141876\n",
            "Train epoch: 10 [40320 / 60032 (67%)]\tLoss: 0.264215\n",
            "Train epoch: 10 [42240 / 60032 (70%)]\tLoss: 0.388585\n",
            "Train epoch: 10 [44160 / 60032 (74%)]\tLoss: 0.087136\n",
            "Train epoch: 10 [46080 / 60032 (77%)]\tLoss: 0.130380\n",
            "Train epoch: 10 [48000 / 60032 (80%)]\tLoss: 0.121312\n",
            "Train epoch: 10 [49920 / 60032 (83%)]\tLoss: 0.103557\n",
            "Train epoch: 10 [51840 / 60032 (86%)]\tLoss: 0.030002\n",
            "Train epoch: 10 [53760 / 60032 (90%)]\tLoss: 0.030262\n",
            "Train epoch: 10 [55680 / 60032 (93%)]\tLoss: 0.019584\n",
            "Train epoch: 10 [57600 / 60032 (96%)]\tLoss: 0.070192\n",
            "Train epoch: 10 [59520 / 60032 (99%)]\tLoss: 0.104240\n",
            "\n",
            "Test set: Average loss: 0.1087, Accuracy: 9675/10000 (97%)\n",
            "\n",
            "CPU times: user 9min 53s, sys: 39 s, total: 10min 32s\n",
            "Wall time: 10min 32s\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}